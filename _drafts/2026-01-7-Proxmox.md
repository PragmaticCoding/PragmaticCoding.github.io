---
title:  "Getting Started With Proxmox"
date:   2025-10-20 12:00:00 -0500
categories: homelab
logo: /assets/logos/JavaFXLogo.png
permalink: /homelab/proxmox
M910Q: /assets/homelab/M910Q-Front-Back.png
Stack: /assets/homelab/M910Q-stack.jpg
M910_Inside_Top: /assets/homelab/M910Q-top.jpg
M910_Inside_Bottom: /assets/homelab/M910Q-bottom.jpg
M910Q_Specs: /assets/homelab/ThinkCentre_M910_Tiny_Spec.PDF
M710Q_Specs: /assets/homelab/ThinkCentre_M710_Tiny_Spec.PDF
Stack1k: /assets/homelab/ThinkCentre_M710_Tiny_Spec.PD
Stack1: /assets/homelab/M910Q-Front-Back.png



Diagram: /assets/elements/ListProperties.png
OLArticle: /javafx/elements/observable-classes-lists
ABGuide: /beginners/intro
GitHubLink: https://github.com/thomasnield/ConstrainedProperty
InstallationLink: https://pve.proxmox.com/pve-docs/chapter-pve-installation.html

JavaDocOL: https://openjfx.io/javadoc/23/javafx.base/javafx/collections/ObservableList.html
JavaDocFXC: https://openjfx.io/javadoc/23/javafx.base/javafx/collections/FXCollections.html

excerpt: "A look at the Lenovo M910Q Tiny computers I've chosen as homelab servers."
---

# Introduction

If you are going to self-host a number of services, you are going to need a number of servers to provide them.  While it is possible - and sometimes advisable - to run multiple services on a single server, it's often best to run each service on its own server.  This avoids conflicts over server resources between services and can make it easier to manage.  This is especially true when each service installation is an exercise in trial-and-error to configure things properly - and this is probably what you are going to experience.

However, buying and setting up a physical server for each service can quickly become expensive, time consuming and consume a lot of power.  This is where "virtual" servers come into play.

## Virtual Servers

A working computer is some kind of hardware with an operating system running on it.  The operating system sits between the hardware and your application software and handles all of the access to the system resources on the hardware.  This includes the CPU, the memory, the USB devices, the monitors, keyboard and networking, along with a lot more.  You may need a different version of an application to run on each type of operationg system, but you generally don't need different versions to run on different hardware.  The operating system handles that for you.

The operating system itself, however, is just software that's been stored on disk, configured for the particular hardware and then loaded at boot time.  You could have many different operating systems stored on a computer and then pick which ever one you want to load when you boot the computer.  You can clone a computer, including all of the application software that has been installed on it, by simply coping the operating system onto a new disk in a second computer and then booting that second computer from it.  This will work as long as the hardware is close enough between the two computers.

You should be able to see that a "computer" is actually just data that installed onto some hardware.  You can treat it as data.

It turns out that you can write software that sits between on operating system and the hardware that it's running on, and that pretends to be the hardware to that operating system.  Now the operating system isn't interacting with the hardware directly, it's interacting with this new layer of software.  

This new layer of "in between" software becomes the actual operating system that the computer is running. It's the piece that actually interacts with hardware of the computer.  The operating system that it is hosting is now divorced from the hardware and it's not a physical computer any more - we call it a "Virtual Machine".

From inside the virtual machine, it's almost impossible to tell that it's not running directly on a physical computer.  From outside the virtualized environment, it's equally as difficult to determine whether or not the server that you are connecting to is physical or virtual.

What's even more important, is that this virtualized environment can run more than one virtual machine at the same time.  In fact, it can run as many virtual machines as you like, until the point where the physical resources of the hardware are over-taxed.

The software that provides this virtualized environment is called a "Hypervisor".  From Wikipedia:

> The term hypervisor is a variant of supervisor, a traditional term for the kernel of an operating system: the hypervisor is the supervisor of the supervisors,[2] with hyper- used as a stronger variant of super-

## Hypervisor Software

Hypervisors have been around for a long time, but started to become extremely common in the 21st century.  

At the last place that I worked, we started out with a very small number of physical servers in 1995, and then kept adding physical servers over time.  Eventually this got out of hand, especially for the small IT shop that we were.  At some point we purchased about 3 "blade" servers to run everything.  These were multi-socket systems with either 4 or 8 cpu chips and, if I remember correctly, about 196GB of memory.  We converted virtually all of our physical servers to virtual servers and ran everything off these three blades.  Eventually, we moved from our own hardware to cloud-based computing.  But it was still largely the same thing, virtual machines running in a cloud-based hypervisor.

Back in those days, there were really just two viable alternatives.  VMWare and Microsoft's Hyper-V.  VMWare was a bit more mature, but both worked well.  Neither was particulaly cheap for commercial use, IIRC, although both had free (or nearly free) versions for non-commercial uses.  Like homelabs.

My understanding is that after being aquired by Broadcom, VMWare has made a number of pricing and licensing changes that have driven away a number of homelab users.  They also dropped support for EIAx, which many homelab users relied on.

From what I can see, VMWare is still widely used but may be losing some of its user base.  Hyper-V is (I believe) Windows based, so appeals mostly to those heavily invested in a Windows infrastructure (not me!), and has a smaller user base.  

Many people are now using Proxmox, which is what I have chosen to go with.  It's free and opensource, and has more than enough capabilities and features to do whatever I need.  

# Installing Proxmox

I've done this 4 times now and found the installation process to be trivial.  The actual steps are dependent on the hardware that you have, so I'll restrict myself to a broad description of the process, and let you read the detail from the [Proxmox website]({{page.InstallationLink}}).

Create a boot image on a USB drive

: This is dead simple if you are using Linux as I do.  If you are using Windows then you'll probably have to install Etcher or Rufus to create a bootable USB thumbdrive.  In Linux, you'll probably have a compatible utility already installed.

Boot your computer to the BIOS Screen

: You'll need to boot your Proxmox host computer twice.  Hook up a monitor and a keyboard, and maybe a mouse, plug in a cable between the ethernet port and a network switch and turn the computer on.  Start hammering on whatever keys your computer needs to bring up the BIOS menu.  Then go into the BIOS settings and make sure that the hardware virtualizing is turned on.  What this will be called will depend on the system.  Proxmox indicates that for Intel based systems this will be called "Intel VT" and for AMD based ones it will called "AMD-V".  It might also jus be referred to as "KVM" virtualization.

Reboot your computer and get to the boot device menu

: Stick your USB drive into a port, save whatever changes that you made to the BIOS, and restart the computer.  Hammer on whichever key wil bring up the "Boot Devices" menu, which may be different than the one in the previous step.  Pick the USB drive.  On some systems, you may not have this menu, so you'll have to configure the boot device order (specify the USB device as the primary) in the BIOS settings.

Follow the instructions in the Proxmox installation screens

: Proxmox won't let you proceed if you haven't turned on the hardware virtualization.  But if you have, then you'll get a menu.  You can go graphical or text, it doesn't really matter.  The options you'll be presented with are fairly straight-forward and obvious, involving time-zones, keyboard layouts, ip addresses and passwords.  You're probably best off picking ext4 as the file system type on your first installation, but that's also the default.  

Take out the USB drive and reboot

: At some point you'll be prompted to do this.  Just follow the instructions.  You don't even need the keyboard, mouse and monitor at this point, as this is now a headless server that you'll manage mostly through it's browser interface.

Log into the managment interface

: Once it's all done, surf over to port 8006 on whatever IP address you specified during the installation process. You'll need to log in using the password you supplied, and then you should see the management interface.  Now you're running Proxmox!

It's possible something might go wrong, but otherwise it really is that simple.

# Proxmox Basics

Let's have a quick look at the features of Proxmox, and how to use them...

## Clustering

If you're like me, you've chosen to use a few inexpensive computers as Proxmox hosts rather than one powerful computer.  In this case, you might as well get them both up and running and then link them together into a cluster.  Once they are in a cluster, you can use a single browser session to see all of the hosts and manage them at the same time.  You can also move your VM's around between the hosts quite easily.

At this point, and certainly if you have less than three hosts, don't attempt to set up "High Availability" on your cluster.

## Creating Virtual Systems

This is the heart and soul of what you'll use Promox for, and everything else is just to support this working smoothly.  Proxmox supports two kinds of virtual guests: Virtual Machines (VM's) and Linux Containers (LXC's)...

### Creating LXC's

I'm going to cover this before moving on to VM's because, in my opinion, LXC's are just a better fit for most applications.  In fact, I won't use a VM unless whatever application that I'm trying to install demands one.

In a nutshell, an LXC is a lightweight VM that shares its kernel with the host machine.  This has the following advantages:

1. They are smaller than VM's and consume less resources because they don't have their own kernel.
1. They use less system resources since they don't have their own kernel.
1. They use system resources more dynamically.
1. They "spin up" in seconds, even when brand new.

However, some people don't like them because:

1. They are not truly isolated from the host o/s and other LXC's.  If your LXC becomes compromised in some way, it could compromise your host, and any other LXC's on that host.
1. If they cause a kernel panic and crash, it's your host kernel that's panicked and your host that's crashed.

The other thing to note is that, since they share the host's kernel, they can only be Linux systems.  If you want to run a Windows instance, you'll have to use a true VM.

I'll leave it up to you to do your own research and decide if LXC's are something that you want to use.

Linux containers come in two flavours...

#### Privileged vs Unprivileged Containers

When you create an LXC, you'll come across an checkbox that says "Not privileged", and it's default value is "true", or, if you like, "checked".  This is very important.

The difference is actually quite simple.  In a privileded container, the "root" user is the same as in the host.  This means that the root user in the container has full access to everything in the host that "root" has in the host.  This means devices and files.  Everything.

In an unprivileged container, the "root" user is not the same user as in the host.  This means that it does not have universal access to the host resources.

Unless you have a very specific reason to do otherwise, create every LXC instance as "unprivileged".  99% of the time, this will have no impact anything you want to do inside your container.  However, you won't be able to do some things like create NFS mounts, that require root access to networking resources.  There are often ways around this that don't involve making your LXC privileged.

Once you've spun up an LXC it's virtually indestinguishable from a regular VM from inside the container.  You can use "apt" to install software, you can execute any Linux command.  It's essentially the same as having a full-blown VM, or even a physical Linux server.

#### The Process

At the top right corner of the main screen in Proxmox there's a button that says "Create CT".  Click it and it will bring up a dialogue box that runs through a creation workflow.  There are 5 tabs that you'll need to work through.  The first looks like this:

### Creating VM's


### Using Community Scripts


### Creating Docker Containers
